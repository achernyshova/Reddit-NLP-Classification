{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this step to collect posts from any 2 subreddits. <br>\n",
    "For my project I chose sport topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libriaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for parsing subreddits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For collecting data I've defined 2 functions. <br> \n",
    "\n",
    "Load_posts - accepts list where to collect data, direction: 'after'/'before', limit (up to 100) and url as arguments. The function creates request to reddit's API and parses posts from Reddit's JSON. <br>\n",
    "\n",
    "Load_subreddit - accepts name of subreddit as an argument, checks if there is a file with posts of selected subreddit, if there is no file the function parses all available posts from the subreddit, if there is a file the function parses only last posts from subreddit and delete duplicates. Then creates DataFrame and saves it as CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_posts(posts, direction, limit, url):\n",
    "    headers = {'User-agent': 'Bleep bot 0.1'}\n",
    "    pagingId = None\n",
    "    #create while loop, it'll be work until 'after'/'before' gets None\n",
    "    #it allows me to avoid collecting duplicates \n",
    "    while True:\n",
    "        #setting direction 'after'/'before' equal to none\n",
    "        if pagingId == None:\n",
    "            params = {'limit': limit}\n",
    "        else:\n",
    "            params = {direction: pagingId, 'limit': limit}\n",
    "        #create request\n",
    "        res = requests.get(url, params = params, headers=headers)\n",
    "        #if we don't have errors we collect posts until 'after'/'before' gets None again.  \n",
    "        if res.status_code == 200:\n",
    "            the_json = res.json()\n",
    "            posts.extend(the_json['data']['children'])\n",
    "            if the_json['data'][direction] == None:\n",
    "                break;\n",
    "            pagingId = the_json['data'][direction]\n",
    "        #if we get an error break the loop and print code of an error\n",
    "        else:\n",
    "            print(res.status_code)\n",
    "            break\n",
    "        time.sleep(3)\n",
    "\n",
    "def load_subreddit(name):\n",
    "    posts = [] #create empty list for collecting data\n",
    "    url = 'https://www.reddit.com/r/' + name + '/.json' #create url using an argument name\n",
    "    #check if there is a file with posts of the subreddit\n",
    "    #if 'no file' parse all available posts and create new dataframe  \n",
    "    if os.path.exists('../data/'+ name + '.csv') == False:  \n",
    "        load_posts(posts, 'after', 100, url)\n",
    "        df = pd.DataFrame([p['data'] for p in posts]).drop_duplicates(subset='name')\n",
    "    #if there is a file\n",
    "    #load file, parse new posts, add new posts to existed posts and delete duplicates \n",
    "    else:\n",
    "        old_posts_df = pd.read_csv('../data/'+ name + '.csv')\n",
    "        old_posts_df.drop(['Unnamed: 0'], axis=1,inplace=True)\n",
    "        load_posts(posts, 'before', 25, url)\n",
    "        new_posts_df = pd.DataFrame([p['data'] for p in posts]).drop_duplicates(subset='name')\n",
    "        df = pd.concat([old_posts_df,new_posts_df],sort=False).drop_duplicates(subset='name')\n",
    "    #save data to csv\n",
    "    df.to_csv('../data/'+ name + '.csv')\n",
    "    #check how many posts we have\n",
    "    print(name, df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of topics I'd like to parse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sport_topics = ['nba', 'baseball', 'soccer','mls', 'hockey', 'mma', 'boxing', 'FIFA']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nba (577, 100)\n",
      "baseball (920, 103)\n",
      "soccer (826, 100)\n",
      "mls (951, 104)\n",
      "hockey (948, 100)\n",
      "mma (924, 105)\n",
      "boxing (995, 104)\n",
      "FIFA (601, 98)\n"
     ]
    }
   ],
   "source": [
    "for x in sport_topics:\n",
    "    load_subreddit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nba (677, 100)\n",
      "baseball (1032, 104)\n",
      "soccer (924, 100)\n",
      "mls (1050, 104)\n",
      "hockey (1033, 100)\n",
      "mma (1019, 105)\n",
      "boxing (1044, 104)\n",
      "FIFA (703, 98)\n"
     ]
    }
   ],
   "source": [
    "for x in sport_topics:\n",
    "    load_subreddit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nba (667, 100)\n",
      "baseball (1024, 103)\n",
      "soccer (918, 100)\n",
      "mls (1042, 104)\n",
      "hockey (1024, 100)\n",
      "mma (1015, 105)\n",
      "boxing (1042, 104)\n",
      "FIFA (696, 98)\n"
     ]
    }
   ],
   "source": [
    "for x in sport_topics:\n",
    "    load_subreddit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "climateskeptics (983, 99)\n"
     ]
    }
   ],
   "source": [
    "load_subreddit('climateskeptics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Futurology (913, 103)\n"
     ]
    }
   ],
   "source": [
    "load_subreddit('Futurology')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShowerThoughts (764, 100)\n"
     ]
    }
   ],
   "source": [
    "load_subreddit('ShowerThoughts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AskReddit (994, 97)\n"
     ]
    }
   ],
   "source": [
    "load_subreddit('AskReddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AskScience (1000, 97)\n"
     ]
    }
   ],
   "source": [
    "load_subreddit('AskScience')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History (998, 99)\n"
     ]
    }
   ],
   "source": [
    "load_subreddit('History')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In order to collect more data I automated collection. I put scrip to AWS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://praw.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to collect more data I automated collection. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

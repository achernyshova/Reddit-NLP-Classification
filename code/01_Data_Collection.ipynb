{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal for this project collecting data by scraping Reddit website and then building a binary classifier to identify where a given post came from. <br> \n",
    "For the project I chose highly correlated subreddits:\n",
    " - Dog lovers subreddit - Dogs https://www.reddit.com/r/dogs/\n",
    " - Dog haters subreddit - Dogfree https://www.reddit.com/r/Dogfree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *This model could be used for identifying dog lovers and dog haters based on their posts on different social networks. And this information could be used further for ads targeting.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this step to collect enough posts from chosen 2 subreddits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libriaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for parsing subreddits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was gathered from Reddit's API, using the Python requests library. Reddit's API returns a JSON file with the pageâ€™s content.\n",
    "\n",
    "I've defined 2 functions for collecting data. <br> \n",
    "\n",
    "**Load_posts** - accepts list where to collect data, direction: 'after'/'before', limit (up to 100) and url as arguments. The function creates request to reddit's API and parses posts from Reddit's JSON. <br>\n",
    "\n",
    "**Load_subreddit** - accepts name of subreddit as an argument, checks if there is a file with posts of selected subreddit, if there is no file the function parses all available posts from the subreddit, if there is a file the function parses only last posts from subreddit and delete duplicates. Then creates DataFrame and saves it as CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_posts(posts, direction, limit, url):\n",
    "    headers = {'User-agent': 'Bleep bot 0.1'}\n",
    "    pagingId = None\n",
    "    #create while loop, it'll be work until 'after'/'before' gets None\n",
    "    #it allows me to avoid collecting duplicates \n",
    "    while True:\n",
    "        #setting direction 'after'/'before' equal to none\n",
    "        if pagingId == None:\n",
    "            params = {'limit': limit}\n",
    "        else:\n",
    "            params = {direction: pagingId, 'limit': limit}\n",
    "        #create request\n",
    "        res = requests.get(url, params = params, headers=headers)\n",
    "        #if we don't have errors we collect posts until 'after'/'before' gets None again.  \n",
    "        if res.status_code == 200:\n",
    "            the_json = res.json()\n",
    "            posts.extend(the_json['data']['children'])\n",
    "            if the_json['data'][direction] == None:\n",
    "                break;\n",
    "            pagingId = the_json['data'][direction]\n",
    "        #if we get an error break the loop and print code of an error\n",
    "        else:\n",
    "            print(res.status_code)\n",
    "            break\n",
    "        #add 3 seconds to delay request in order to follow API access rules (up to 60 requests per minute)    \n",
    "        time.sleep(3) \n",
    "\n",
    "def load_subreddit(name):\n",
    "    posts = [] #create empty list for collecting data\n",
    "    url = 'https://www.reddit.com/r/' + name + '/new/.json' #create url using an argument name\n",
    "    #check if there is a file with posts of the subreddit\n",
    "    #if 'no file' parse all available posts and create new dataframe  \n",
    "    if os.path.exists('../data/'+ name + '.csv') == False:  \n",
    "        load_posts(posts, 'after', 100, url)\n",
    "        df = pd.DataFrame([p['data'] for p in posts]).drop_duplicates(subset='name')\n",
    "    #if there is a file\n",
    "    #load file, parse new posts, add new posts to existed posts and delete duplicates \n",
    "    else:\n",
    "        old_posts_df = pd.read_csv('../data/'+ name + '.csv')\n",
    "        old_posts_df.drop(['Unnamed: 0'], axis=1,inplace=True)\n",
    "        load_posts(posts, 'before', 25, url)\n",
    "        new_posts_df = pd.DataFrame([p['data'] for p in posts]).drop_duplicates(subset='name')\n",
    "        df = pd.concat([old_posts_df,new_posts_df],sort=False).drop_duplicates(subset='name')\n",
    "    #save data to csv\n",
    "    df.to_csv('../data/'+ name + '.csv')\n",
    "    #check how many posts we have\n",
    "    print(name, df.shape)\n",
    "    return (name, df.shape[0]) #name of subreddit and count of posts there for stats file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of topics I'd like to parse. Before I chose final 2 subreddits, I collected data from different subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sport_topics = ['nba', 'baseball', 'soccer','mls', 'hockey', 'mma', 'boxing', 'FIFA']  \n",
    "other_topics = ['news', 'Futurology','AskEngineers','AskReddit','AskScience','History','gameofthrones','gottheories','Dogfree','aww','dogs']\n",
    "tech_topics =  ['apple','applehate','android','MacSucks','mac','iphone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nba (4838, 100)\n",
      "baseball (2032, 104)\n",
      "soccer (3099, 100)\n",
      "mls (1377, 104)\n",
      "hockey (1535, 100)\n",
      "mma (1650, 106)\n",
      "boxing (1209, 104)\n",
      "FIFA (2582, 98)\n",
      "news (339, 97)\n",
      "Futurology (1336, 103)\n",
      "AskEngineers (1179, 99)\n",
      "AskReddit (10526, 97)\n",
      "AskScience (1227, 97)\n",
      "History (1089, 99)\n",
      "gameofthrones (1114, 101)\n",
      "gottheories (1014, 103)\n",
      "Dogfree (1119, 104)\n",
      "aww (6809, 102)\n",
      "dogs (1285, 100)\n",
      "apple (1027, 103)\n",
      "applehate (61, 98)\n",
      "android (878, 101)\n",
      "MacSucks (166, 95)\n",
      "mac (1271, 104)\n",
      "iphone (1087, 103)\n"
     ]
    }
   ],
   "source": [
    "for x in sport_topics + other_topics + tech_topics:\n",
    "    load_subreddit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to collect more data I automated collection. I put scrip 'reddit_collect.py' to AWS E2 instance and run every hour using cron task<br> \n",
    "\n",
    "(0 * * * * /home/ubuntu/anaconda3/bin/python /home/ubuntu/project/reddit_collect.py).<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To control AWS data collection I defined a function for saving statistics and added it to my script 'reddit_collect.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_stat(stats):\n",
    "    f = open('../data/stat.txt','a+') #create new file or open and add new rows to the file \n",
    "    f.write('***********' + str(time.ctime()) + os.linesep) #add time when data collects\n",
    "    for stat in stats:\n",
    "        name = stat[0]\n",
    "        count = str(stat[1])\n",
    "        f.write(name +', ' + count + os.linesep) #add subreddit's name and total number of posts in my files\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code I changed in 'reddit_collect.py' to save statiscitcs \n",
    "stats = []\n",
    "for x in sport_topics + other_topics + tech_topics:\n",
    "    stats.append(load_subreddit(x))\n",
    "save_stat(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - I collected around 1200 post post for each of chosen Subreddits (Dogs and Dogfree)\n",
    " - The biggest number of posts I collected from AskReddit - around 11,000 posts \n",
    " - Automated collection using ASW and Cron\n",
    " \n",
    "Further improvements for the data collection:\n",
    " - Use PRAW: The Python Reddit API Wrapper for parsing data from Reddit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
